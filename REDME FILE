# ğŸ“„ **README â€“ Blackcoffer Data Extraction & NLP Analysis**

## ** Project Overview**

This project completes the **Blackcoffer â€“ Data Extraction & Text Analysis Assignment**.
It performs:

1. **Web Scraping** â€“ Extracts only the *article title and article text* from each URL provided in `Input.xlsx`.
2. **Text Processing & NLP** â€“ Computes all required linguistic and readability metrics from the extracted article text.
3. **Output Generation** â€“ Saves:

   * All extracted articles as `.txt` files
   * Final results in the required format (`Output.xlsx`)

This project is fully implemented using **Python** as instructed.

---

## ** Project Structure**

```
/Your_Submission/
â”‚â”€â”€ blackcoffer_assignment.py
â”‚â”€â”€ Input.xlsx
â”‚â”€â”€ Output.xlsx
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ /MasterDictionary/
â”‚      â”œâ”€â”€ positive-words.txt
â”‚      â””â”€â”€ negative-words.txt
â”‚â”€â”€ /extracted_articles/
       â”œâ”€â”€ 1.txt
       â”œâ”€â”€ 2.txt
       â”œâ”€â”€ ...
```

---

## **ğŸ› ï¸ Requirements**

Install all dependencies using:

```bash
pip install -r requirements.txt
```

### **Python Libraries Used**

* requests
* beautifulsoup4
* pandas
* nltk
* openpyxl
* argparse
* re
* os

NLTK stopwords are auto-downloaded inside the script.

---

## **ğŸš€ How to Run the Script**

Open terminal / command prompt in the project directory and run:

```bash
python blackcoffer_assignment.py --input Input.xlsx --output Output.xlsx --text_dir extracted_articles
```

### **Arguments Explained**

| Argument     | Description                                    |
| ------------ | ---------------------------------------------- |
| `--input`    | Path to Input.xlsx                             |
| `--output`   | Path where Output.xlsx should be generated     |
| `--text_dir` | Folder name to save all extracted `.txt` files |

Example:

```bash
python blackcoffer_assignment.py --input Input.xlsx --output Output.xlsx --text_dir extracted_articles
```

---

## ** What the Script Does**

###  **Data Extraction**

For each URL in Input.xlsx:

* Fetches the webpage using `requests`
* Extracts:

  * Article Title (`<h1>`)
  * Article Body (`<div class="td-post-content">`)
* Removes website header, footer, ads, menus, etc.
* Saves the extracted article as a `.txt` file with the name:

  ```
  URL_ID.txt
  ```

---

###  **Text Analysis**

The script computes all variables listed in **Text Analysis.docx**, including:

* Positive Score
* Negative Score
* Polarity Score
* Subjectivity Score
* Average Sentence Length
* Percentage of Complex Words
* Fog Index
* Average Words per Sentence
* Complex Word Count
* Word Count
* Syllables per Word
* Personal Pronouns
* Average Word Length

Complex words are identified using a custom syllable-counting function.

---

###  **Output Generation**

Creates **Output.xlsx** with:

* All input columns (URL_ID, URL, etc.)
* All computed NLP features
* Extracted Title

---

## ** Dependencies / Dictionary Files**

The script uses:

```
MasterDictionary/positive-words.txt
MasterDictionary/negative-words.txt
```

Ensure both files are included in the same folder structure.

---

## ** Notes**

* Internet connection is required for web scraping.
* If some pages do not load, the script logs errors and continues.
* All text cleaning and processing follow the guidelines in the assignment.

---

## ** Submission Checklist**

Before uploading to Google Drive, ensure you include:

* âœ” `blackcoffer_assignment.py`
* âœ” `Output.xlsx`
* âœ” All extracted `.txt` files
* âœ” `README.md`
* âœ” `requirements.txt`
* âœ” `MasterDictionary` folder

---

âœ… `requirements.txt`
âœ… Final Submission ZIP Folder


